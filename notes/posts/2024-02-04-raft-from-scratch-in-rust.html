<!-- -*- mode: markdown -*- -->
# An intuition for distributed consensus in OLTP systems
## February 7, 2024
###### raft,distributed consensus,draft

After implementing Raft [in
Go](https://notes.eatonphil.com/2023-05-25-raft.html) last year,
running a book club reading [Designing Data Intensive
Applications](https://eatonphil.com/2023-ddia.html), running another
book club reading [Database
Internals](https://eatonphil.com/2023-database-internals.html),
implementing Raft a [second
time](https://github.com/eatonphil/raft-rs) in Rust this year; and
spending the last few years at companies built on distributed
consensus, there's been a lot to digest and a lot to characterize.

This post is an attempt to share some of the intuition built up
reading about and working in this space. Originally this post was also
going to end with a walkthrough of my most recent Raft implementation
in Rust. But I'm going to hold off on that for another time.

I'm not an expert on these topics, but I was fortunate to have a few
experts review versions of this post: Paul Nowoczynski, Alex Miller,
Jack Vanlightly, Daniel Chia, and Alex Petrov.

### It's about availability

Distributed consensus helps a group of nodes, a cluster, agree on
a value. A client of the cluster can treat a value from the cluster,
with some care we'll describe later, as if the value was atomically
written to and read from a single thread. This property is called
[linearizability](https://jepsen.io/consistency/models/linearizable).

However, with distributed consensus, the client of the cluster has
better availability guarantees from the cluster than if the client
atomically wrote to or read from a single thread. A single thread that
crashes becomes unavailable. But some number `f` nodes can crash in a
cluster implementing distributed consensus and still 1) be available
and 2) provide linearizable reads and writes.

That is: <b>distributed consensus solves the problem of high
availability for a system while remaining linearizable</b>.

Without distributed consensus you can still achieve high
availability. For example, a database might have two read
replicas. But a client reading from a read replica might get stale
data. Thus, this system (a database with two read replicas) is not
linearizable.

Without distributed consensus you can also try synchronous
replication. It would be very simple to do. But the value here is
extremely limited. If a single node in the cluster goes down the
entire cluster is down.

You might think I'm proposing a strawman. We could simply designate a
permanent leader that handles all reads and writes; and require a
majority of nodes to commit a message before the leader responds to a
client. But in that case, what's the process for getting a lagging
follower up-to-date? And what happens if it is the leader who goes
down?

Well, these are not trivial problems! And, beyond linearizability that
we already mentioned, these problems are exactly what distributed
consensus solves.

### Why does linearizability matter?

It's very nice, and often even critical, to have a highly available
system that will never give you stale data. And regardless, it's
convenient to have a term for what we might naively think of as the
"correct" way you'd always want to set and get a value.

So linearizability is a convenient way of thinking about complex
systems, if you can use or build a system that supports it. But it's
not the only consistency approach you'll see in the wild.

As you increase the guarantees of your consistency model, you tend to
sacrifice performance. Going the opposite direction, some production
systems sacrifice consistency to improve performance. For example, you
might allow stale reads from a replica so that you can reduce load on
a leader.

There are formal definitions for lower consistency models, including
sequential and read-your-writes. You can read the [Jepsen
page](https://jepsen.io/consistency) for more detail.

### Raft

Raft is a distributed consensus algorithm that allows you to build a
replicated state machine on top of a replicated log.

It has a semi-permanent leader that all reads and writes go through
(if you want to remain linearizable). Processes in the cluster pick a
leader among themselves. Clients ask all nodes in the cluster to add a
new message to the log. Only the leader accepts this message, if there
is currently a leader. Clients retry until there is a leader that
accepts the message.

The leader appends the message to its log and makes sure to replicate
all messages in its log to followers in the same order. The leader
sends periodic heartbeat messages to all followers to prolong its term
as leader. If a follower hasn't heard from the leader within a period
of time, it becomes a candidate and requests votes from the cluster.

When a follower is asked to accept a new message, it checks if its
history is up-to-date with the leader. If it is not, the follower asks
the leader to send previous messages to bring it up-to-date. It does
this, in the worst case of a follower that completely lost all
history, by ultimately going all the way back to the very first
message ever sent.

When a quorum (basically, a majority) of nodes has accepted a message,
the leader marks the message as committed and applies the message to
its own state machine. When followers learn about newly committed
entries, they also apply committed entries to their own state machine.

These details and more can be mostly be found in Figure 2 of the Raft paper.

![/raft-figure2.png](/raft-figure2.png)

### Best and worst case scenarios

- Messages always delivered quickly and correctly
- Reads from and writes to disk are always complete and correct

### What happens when you add nodes?

Distributed consensus algorithms make sure that some minimum number of
nodes in a cluster agree before continuing. The minimum number is
proportional to the total number of nodes in the cluster.

A typical implementation of Raft for example will require 3 nodes in a
5-node cluster to agree before continuing. 4 nodes in a 7-node
cluster. And so on.

Recall that the p99 latency for a service is at least as bad as the
slowest external request the service must make. As you increase the
number of nodes you must talk to in a consensus cluster, you increase
the chance of a slow request.

Consider the extreme case of a 101-node cluster requiring 51 nodes to
respond before returning to the client. That's 51 chances for a slower
request. Compared to 4 chances in a 7-node cluster. The 101-node
cluster is certainly more highly available though! It can tolerate 49
nodes going down. The 7-node cluster can only tolerate 3 nodes going
down. The scenario where 49 nodes go down (assuming they're in
different availability zones) seems pretty unlikely!

### Horizontal scaling with distributed consensus? Not exactly

All of this is to say that the most popular algorithms for distributed
consensus, on their own, have nothing to do with horizontal scaling.

The way that horizontally scaling databases like Cockroach or Yugabyte
or Spanner work is by partitioning the data. Within each partition
data is replicated with a dedicated distributed consensus cluster.

So, yes, distributed consensus *can* be a part of horizontal
scaling. But again what it primarily solves is high availability via
replication while remaining linearizable.

This is not a trivial point to
make. [etcd](https://web.archive.org/web/20230327030543/https://etcd.io/docs/v3.2/learning/why/#using-etcd-for-metadata),
[consul](https://web.archive.org/web/20231212132325/https://www.hashicorp.com/resources/operating-and-running-consul-at-scale),
and [rqlite](https://github.com/rqlite/rqlite) are examples of
databases that do not do partitioning, only replication, via a single
Raft cluster that replicates all data for the entire system.

For these databases there is no horizontal scaling. If they support
"horizontal scaling", they support this by doing non-linearizable
(stale) reads. Writes remain a challenge.

This doesn't mean these databases are bad. They are not. One obvious
advantage they have over Cockroach or Spanner is that they are
conceptually simpler. Conceptually simpler often equates to easier to
operate. That's a big deal.

### Everything aside, consensus is expensive

Distributed consensus brings overhead, there's no question about it.
